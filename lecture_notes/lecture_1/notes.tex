\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{pgfplots}

\begin{document}

% PROBLEM
\section{What is a dynamical system?}

There are two classes. ODEs where $\dot{x}=f(x)$ where $\dot{x}=\frac{dx}{dt}$, $x\in\mathbb{R}^n$ and $x(0)=x_0$. Note that dynamical systems $\dot{x}=f(x,t)$ can be converted to $f(x)$ via a trick. There are two types of ODEs, autonomous and non-autonomous systems, where non-autonomous is time-dependent.

Iterative maps are of the form $x_{t+1}=f(x_t)$ where $x_t\in\mathbb{R}^n$ and $t=0,1,2,3,\ldots$.

\section{Do solutions exist}
For ODEs, do solutions even exist? If so, are they unique?

Answer: Usually. For instance, if $f$ is Lipschitz continuous (in a region) then solutions exist (at least loccally) and are unique.

\subsection{Continuity}
$f$ is continuous at $x$ if $f(y)$ is close to $f(x)$ whenever $y$ is close to $x$. "is close to" is not precise. More precisely, $\forall \epsilon>0$, $\exists \delta >0$ such that $|| f(y)-f(x)||< \epsilon$ whenever $||x-y||<\delta$.

\subsection{Lipschitz Continuity}
$f$ is Lipschitz continuous in a region $\Omega$ if $\exists K>0$ such that
\[ ||f(x)-f(y)|| \leq K ||x-y|| \]
for all $x,y\in\Omega$. Lipschitz continuous imples continuous, but continuous does not imply Lipschitz. An example is $f(x)=x^{1/3}$.

If the derivative of $f$ exists and is continuous, this implies Lipschitz via the mean value theorem.

Also note that $\Omega$ has to be compact.

\subsection{Global existence}
Lipschitz continuous functions may not have global existence. For example $\dot{x}=x^2$.
\begin{gather*}
    \dot{x}=x^2\\
    \int_{x_0}^{x(t)}\frac{dx}{x^2}=\int_0^t dt\\
    -\frac{1}{x}\Big|_{x_0}^{x(t)}=t-0\\
    -\frac{1}{x(t)}+\frac{1}{x_0}=t\\
    x(t)=\frac{1}{\frac{1}{x_0}-t}
\end{gather*}
This exibits a finite time blow up as $t\Rightarrow \frac{1}{x_0}$, so there is no global existence of a solution, only local.

\section{Gronwall's Lemma}
Suppose $u(t)\geq 0$ and $C,K\geq 0$ are constants. Suppose 
\[ U(t) \leq C+\int_o^tKu(s) ds \qquad \forall t\in[o,T] \]
then $u(t)\leq Ce^{KT} \qquad \forall t\in[o,T]$.

\subsection{Proof}
Let $U(t) \leq C+\int_o^tKu(s) ds \qquad \forall t\in[o,T]$. So we're given $u(t)\leq U(t)$.
\begin{align*}
    \frac{d}{dt}u(t)\leq K u(t) &\Rightarrow u(t)-u(0)\leq K\int U(t)\\
    \frac{d}{dt}U(t)=K (t) \leq K U(t) &\Rightarrow \frac{d}{dt}\log U(t)\leq K\\
    &\Rightarrow \log U(t) -\log U(0)=K(t-o)\\
    &\Rightarrow U(t)=e^{Kt}\cdot C
\end{align*}
Somehow we integrate to get $u(t)\leq e^{Kt}\cdot c$.

\subsection{Dependence on initial conditions}
Suppose $\dot{x}=f(x)$ and $x(0)=x_0$. We integrate via the fundamental theorem of calculus to get 

\begin{align*}
    x(t)&=x(0)+\int_0^t f(x(s))ds\\
    y(t)&=y(0)+\int_0^t f(y(s)) ds\\
    ||x(t)-y(t) || &\leq ||x(0)-y(0)|| + ||\int_0^t\left[ f(x(s))-f(y(s)) \right] ds \text{ Via triangle inequality}\\
    &\leq ||x(0)-y(0)|| + \int_0^t||\left[ f(x(s))-f(y(s)) \right] ds\\
    &\leq ||x(0)-y(0)|| + \int_0^tK||x(s)-y(s)||ds \text{ Via Lipschitz} \\
\end{align*}
Applying Gronwall Lemma, we get 
\[ ||x(t)-y(x)|| \leq e^{Kt}||x(0)-y(0)|| \]
Given $\epsilon >0$, choose $\delta<\frac{\epsilon}{e^{Kt}}$. Hence, solutions depend continuously on $x(0)$. 

So we have abound, but it is terrible given the exponential growth of the initial condition dependence. Unfortunately, we can't do better and this is the point of chaos.

\subsection{Examples}

\subsubsection{Global existence}
Suppose $\dot{x}=1-x^2$. We have two fixed points at $x\pm 1$, where $x=-1$ is unstable and $x=1$ is stable. For intial conditions $x(0) < -1$, then we get finite time blow up.

Notice that $x\in[-1,1]$ is invariant and $[-1,1]$ is compact. This implies global existence. More generally, $x(0)$ in compact, positively invariant set implies \textit{global} existence.

\subsubsection{An introductory example}
\[ \ddot{x}-x+x^2-\epsilon \left[ \alpha y + \beta xy \right] =0 \]
While this is second order, we can get it into a system of first order ODEs. Let $y=\dot{x}$ and we can write
\begin{align*}
    \dot{x}=y\\
    \dot{y}=x-x^2+\epsilon \left[ \alpha y + \beta xy \right]
\end{align*}
When $\epsilon=0$, equations are Hamiltonian.

A ``Hamiltonian'' equation is $H(x,y)$ such that
\begin{align*}
    \dot{x}&=\frac{\partial H}{\partial y}\\
    \dot{y} &=-\frac{\partial H}{\partial x}\\
    H(x,y)&=\frac{y^2}{2}-\frac{x^2}{2}+\frac{x^3}{3}
\end{align*}
Notablly, $H$ is conserved along trajectories.
\[
\frac{d}{dt}H(x(t),y(t))=\frac{\partial H}{\partial x}\dot{x}+\frac{\partial H}{\partial y}\dot{y}=\frac{\partial H}{\partial x}\frac{\partial H}{\partial y}+\frac{\partial H}{\partial x}(-\frac{\partial H}{\partial y})=0
\] 
Hence,
\[ H(x(t),y(t))=C\]
and solutions move along level set of $H$ ($H=$constant). When $H=0$, we get a ``separatrix'' that deterimines whether our trajectory falls into qualitatively distinct regions.

Okay, now we have two fixed points and we can linearize around them.

\end{document}