\documentclass{article}
\usepackage{amsmath, amssymb}
\usepackage{pgfplots}

\begin{document}

% PROBLEM
\section{What is a dynamical system?}

There are two classes. ODEs where $\dot{x}=f(x)$ where $\dot{x}=\frac{dx}{dt}$, $x\in\mathbb{R}^n$ and $x(0)=x_0$. Note that dynamical systems $\dot{x}=f(x,t)$ can be converted to $f(x)$ via a trick. There are two types of ODEs, autonomous and non-autonomous systems, where non-autonomous is time-dependent.

Iterative maps are of the form $x_{t+1}=f(x_t)$ where $x_t\in\mathbb{R}^n$ and $t=0,1,2,3,\ldots$.

\section{Do solutions exist}
For ODEs, do solutions even exist? If so, are they unique?

Answer: Usually. For instance, if $f$ is Lipschitz continuous (in a region) then solutions exist (at least loccally) and are unique.

\subsection{Continuity}
$f$ is continuous at $x$ if $f(y)$ is close to $f(x)$ whenever $y$ is close to $x$. "is close to" is not precise. More precisely, $\forall \epsilon>0$, $\exists \delta >0$ such that $|| f(y)-f(x)||< \epsilon$ whenever $||x-y||<\delta$.

\subsection{Lipschitz Continuity}
$f$ is Lipschitz continuous in a region $\Omega$ if $\exists K>0$ such that
\[ ||f(x)-f(y)|| \leq K ||x-y|| \]
for all $x,y\in\Omega$. Lipschitz continuous imples continuous, but continuous does not imply Lipschitz. An example is $f(x)=x^{1/3}$.

If the derivative of $f$ exists and is continuous, this implies Lipschitz via the mean value theorem.

Also note that $\Omega$ has to be compact.

\subsection{Global existence}
Lipschitz continuous functions may not have global existence. For example $\dot{x}=x^2$.
\begin{gather*}
    \dot{x}=x^2\\
    \int_{x_0}^{x(t)}\frac{dx}{x^2}=\int_0^t dt\\
    -\frac{1}{x}\Big|_{x_0}^{x(t)}=t-0\\
    -\frac{1}{x(t)}+\frac{1}{x_0}=t\\
    x(t)=\frac{1}{\frac{1}{x_0}-t}
\end{gather*}
This exibits a finite time blow up as $t\Rightarrow \frac{1}{x_0}$, so there is no global existence of a solution, only local.

\section{Gronwall's Lemma}
Suppose $u(t)\geq 0$ and $C,K\geq 0$ are constants. $\forall t\in[o,T] $, suppose 
\[ u(t) \leq C+\int_o^tKu(s) ds \]
then $\forall t\in[o,T]$, we can write $u(t)\leq Ce^{KT}$.

\subsection{Proof}
\subsubsection{Class notes}
Let $U(t) \leq C+\int_o^tKu(s) ds \qquad \forall t\in[o,T]$. So we're given $u(t)\leq U(t)$.
\begin{align*}
    \frac{d}{dt}u(t)\leq K u(t) &\Rightarrow u(t)-u(0)\leq K\int U(t)\\
    \frac{d}{dt}U(t)=K (t) \leq K U(t) &\Rightarrow \frac{d}{dt}\log U(t)\leq K\\
    &\Rightarrow \log U(t) -\log U(0)=K(t-o)\\
    &\Rightarrow U(t)=e^{Kt}\cdot C
\end{align*}
Somehow we integrate to get $u(t)\leq e^{Kt}\cdot c$.

\subsubsection{Corrected Proof}
\textbf{Step 1:} Define the auxiliary function
\[ U(t) := C + \int_o^t K u(s) ds \]
and note that
\begin{enumerate}
    \item $U(0)=C$
    \item $u(t)\leq U(t)$ by assumption
    \item $U(t)$ is differentiable via the fundamental theorem of calculus
\end{enumerate}

\textbf{Step 2:} Differentiate $U(t)$ to get
\[ \frac{d}{dt}U(t) = Ku(t) \]

\textbf{Step 3:} Use the fact that $u(t)\leq U(t)$ to write
\[ \frac{d}{dt}U(t)=K u(t) \leq K U(t) \]

\textbf{Step 4:} Solve this differential inequlity. Assuming $U(t)>0$, we can write
\[ \frac{U'(t)}{U(t)} \leq K \]
Integrating from 0 to $t$, we have
\begin{align*}
    \int_0^t \frac{U'(t)}{U(t)} ds &\leq \int_0^t K ds \\
    \ln U(t) - \ln U(0) &\leq Kt \\
    \ln\left( \frac{U(t)}{C} \right) &\leq Kt \\
    U(t) \leq Ce^{Kt}
\end{align*}

\textbf{Step 5:} Since $u(t)\leq U(t)$, we can conclude that 
\[ u(t)\leq Ce^{Kt} \]

\subsubsection{Post Class Thoughts}
Gronwall's lemma makes a lot of sense. It's just saying if your equation is bounded by the perfect accumulation of it's history, then it is bounded by the solution to the perfect accumulation, wich is just the exponential function.

Another way to say this is that if your non-negative equation is anything less than a perfect accumulation of its previous values, then it is bounded by the exponential function. In general, the exponential function is exactly the solution to when you have perfect feedback.

\subsection{Dependence on initial conditions}
Suppose $\dot{x}=f(x)$ and $x(0)=x_0$. We integrate via the fundamental theorem of calculus to get 

\begin{align*}
    x(t)&=x(0)+\int_0^t f(x(s))ds\\
    y(t)&=y(0)+\int_0^t f(y(s)) ds\\
    ||x(t)-y(t) || &\leq ||x(0)-y(0)|| + ||\int_0^t\left[ f(x(s))-f(y(s)) \right] ds || \text{ Via triangle inequality}\\
    &\leq ||x(0)-y(0)|| + \int_0^t||\left[ f(x(s))-f(y(s)) \right] ds || \text{ See note}\\
    &\leq ||x(0)-y(0)|| + \int_0^tK||x(s)-y(s)||ds \text{ Via Lipschitz} \\
\end{align*}
Applying Gronwall Lemma, we get 
\[ ||x(t)-y(t)|| \leq e^{Kt}||x(0)-y(0)|| \]
Given $\epsilon >0$, choose $\delta<\frac{\epsilon}{e^{Kt}}$. Hence, solutions depend continuously on $x(0)$. 

So we have abound, but it is terrible given the exponential growth of the initial condition dependence. Unfortunately, we can't do better and this is the point of chaos.

\subsubsection{Note}
The inequality
\[ || \int_0^t x(s) || \leq \int_0^t || x(s) || \]
is just a continuous version of the triangle inqueality for summands 
\[ || \sum_{0}^{t} x(s) || \leq \sum_{0}^{t} ||x(s)|| \]

\subsubsection{After class note}
Here is a clean up of the last part of the proof. We want to show that the solution map $\Phi_t : x_0\mapsto x(t)$ is continuous. In other words, for any time $t$ and any $\epsilon > 0$, if we start close enough to $x_0$, we stay within $\epsilon$ of $x(t)$.

Formally, given $\epsilon>0$, we need to find $\delta>0$ such that 
\[ || x_0-y_0 || \Rightarrow ||x(t)-y(t) || < \epsilon \] 
From Gronwall's lemma, we have
\[ || x(t) -y(t) || \leq e^{Kt} ||x_0-y_0|| \] 
so if we choose $\delta=\frac{\epsilon}{e^{Kt}}$, then whenever $||x_0-y_0||<\delta$, we have
\[ ||x(t)-y(t) || \leq e^{Kt} \cdot \delta = e^{Kt} \cdot \frac{\epsilon}{e^{Kt}}=\epsilon \] 
and this proves continuity at each fixed time $t$. This shows that as $t\rightarrow\infty$, then $\delta\rightarrow 0$. Hence, as we ask to be close to solutions farther out in time, we need to find initial conditions exponentially close to the original initial conditions.

\subsection{Examples}

\subsubsection{Global existence}
Suppose $\dot{x}=1-x^2$. We have two fixed points at $x\pm 1$, where $x=-1$ is unstable and $x=1$ is stable. For intial conditions $x(0) < -1$, then we get finite time blow up.

Notice that $x\in[-1,1]$ is invariant and $[-1,1]$ is compact. This implies global existence. More generally, $x(0)$ in compact, positively invariant set implies \textit{global} existence.

\subsubsection{An introductory example}
\[ \ddot{x}-x+x^2-\epsilon \left[ \alpha y + \beta xy \right] =0 \]
While this is second order, we can get it into a system of first order ODEs. Let $y=\dot{x}$ and we can write
\begin{align*}
    \dot{x}=y\\
    \dot{y}=x-x^2+\epsilon \left[ \alpha y + \beta xy \right]
\end{align*}
When $\epsilon=0$, equations are Hamiltonian.

A ``Hamiltonian'' equation is $H(x,y)$ such that
\begin{align*}
    \dot{x}&=\frac{\partial H}{\partial y}\\
    \dot{y} &=-\frac{\partial H}{\partial x}\\
    H(x,y)&=\frac{y^2}{2}-\frac{x^2}{2}+\frac{x^3}{3}
\end{align*}
Notablly, $H$ is conserved along trajectories.
\[
\frac{d}{dt}H(x(t),y(t))=\frac{\partial H}{\partial x}\dot{x}+\frac{\partial H}{\partial y}\dot{y}=\frac{\partial H}{\partial x}\frac{\partial H}{\partial y}+\frac{\partial H}{\partial x}(-\frac{\partial H}{\partial y})=0
\] 
Hence,
\[ H(x(t),y(t))=C\]
and solutions move along level set of $H$ ($H=$constant). When $H=0$, we get a ``separatrix'' that deterimines whether our trajectory falls into qualitatively distinct regions.

Okay, now we have two fixed points and we can linearize around them.

\end{document}